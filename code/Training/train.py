#!/usr/bin/env python3
"""
Training script for the current-action Language-Guided DT (2048-d Qwen embeddings).
History/Strategy tokens are placed before State to keep same-step context visible.
"""
import os
import sys
import numpy as np
import torch
import logging
import time
import argparse
from datetime import datetime

# Add local paths for project modules.
current_dir = os.path.dirname(os.path.abspath(__file__))
code_dir = os.path.abspath(os.path.join(current_dir, ".."))
algo_dir = os.path.join(code_dir, "Algorithms")
sys.path.insert(0, algo_dir)
sys.path.insert(0, code_dir)

from bidding_train_env.common.utils import save_normalize_dict
from language_utils import LanguageAugmentedReplayBufferWithTask, language_collate_fn_with_task
from sembid_DT import LanguageGuidedDTWithTaskFlexible
from torch.utils.data import DataLoader, WeightedRandomSampler

# Configure logging.
root_dir = os.path.abspath(os.path.join(current_dir, "..", ".."))
log_dir = os.path.join(root_dir, "logs")
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'{log_dir}/train_2048.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser(description='Train Language-Guided DT (2048-d, History/Strategy before State)')

    # Language configuration.
    parser.add_argument('--use_language', action='store_true', default=True,
                        help='Enable language guidance')
    parser.add_argument('--language_type', type=str, default='both',
                        choices=['history', 'strategy', 'both', 'none'],
                        help='Language type: history, strategy, both, none')
    parser.add_argument('--language_emb_dim', type=int, default=2048,
                        help='Language embedding dimension')

    # Data configuration.
    parser.add_argument('--data_file', type=str,
                        default='data/train/trajectory_data_2048.pkl',
                        help='Training data path (2048-d precomputed PKL)')

    # Training configuration.
    parser.add_argument('--batch_size', type=int, default=64,
                        help='Batch size')
    parser.add_argument('--num_steps', type=int, default=800000,
                        help='Total training steps')
    parser.add_argument('--save_interval', type=int, default=50000,
                        help='Checkpoint save interval')
    parser.add_argument('--resume_checkpoint', type=str, default=None,
                        help='Checkpoint path for resume')
    parser.add_argument('--start_step', type=int, default=0,
                        help='Resume start step (used with num_steps)')

    # Output configuration.
    parser.add_argument('--output_dir', type=str, default=None,
                        help='Output directory (auto-generated by default)')
    parser.add_argument('--exp_name', type=str, default='',
                        help='Experiment name suffix')

    return parser.parse_args()

def main():
    args = parse_args()

    logger.info("="*70)
    logger.info("Language-Guided DT Training (2048-d, History/Strategy before State)")
    logger.info("="*70)

    # Resolve output directory.
    if args.output_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        exp_name = f"Simbid_Qwen_DT_2048_{timestamp}{args.exp_name}"
        output_dir = os.path.join(root_dir, "models", "LanguageDT", exp_name)
    else:
        output_dir = args.output_dir

    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Output directory: {output_dir}")
    logger.info(f"Language embedding dim: {args.language_emb_dim}")

    # Load data.
    logger.info(f"Loading data: {args.data_file}")
    if not os.path.exists(args.data_file):
        raise FileNotFoundError(f"Data file not found: {args.data_file}")

    # Build dataset.
    dataset = LanguageAugmentedReplayBufferWithTask(
        state_dim=16,
        act_dim=1,
        data_path=args.data_file,
        K=20,
        scale=3000,
        use_language=args.use_language,
        language_type=args.language_type,
        use_precomputed_embeddings=True
    )

    # Build data loader.
    sampler = WeightedRandomSampler(
        weights=torch.ones(len(dataset)),
        num_samples=len(dataset),
        replacement=True
    )

    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        sampler=sampler,
        collate_fn=language_collate_fn_with_task,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )

    # Initialize model.
    logger.info("Initializing model...")
    model = LanguageGuidedDTWithTaskFlexible(
        state_dim=16,
        act_dim=1,
        state_mean=dataset.state_mean,
        state_std=dataset.state_std,
        use_language=args.use_language,
        language_emb_dim=args.language_emb_dim,
        K=20,
        max_ep_len=96,
        scale=3000,
        use_precomputed_embeddings=True
    )

    # Set device.
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    logger.info(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Resume from checkpoint if provided.
    if args.resume_checkpoint:
        if not os.path.exists(args.resume_checkpoint):
            raise FileNotFoundError(f"Resume checkpoint not found: {args.resume_checkpoint}")
        logger.info(f"Loading checkpoint: {args.resume_checkpoint}")
        checkpoint = torch.load(args.resume_checkpoint, map_location=device)
        model.load_state_dict(checkpoint)

    # Optimizer.
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=1e-4,
        weight_decay=1e-4
    )

    # Learning-rate scheduler.
    scheduler = torch.optim.lr_scheduler.LinearLR(
        optimizer,
        start_factor=0.1,
        end_factor=1.0,
        total_iters=10000
    )

    # Training loop.
    logger.info(f"Start training. Total steps: {args.num_steps}")
    model.train()

    # Create a persistent data iterator.
    data_iter = iter(dataloader)

    for step in range(args.start_step, args.num_steps):
        start_time = time.time()

        # Fetch a batch.
        try:
            batch = next(data_iter)
            states, actions, rewards, dones, rtg, timesteps, mask, task_desc_emb, history_emb, strategy_emb = batch
        except StopIteration:
            # Recreate the iterator when exhausted.
            data_iter = iter(dataloader)
            batch = next(data_iter)
            states, actions, rewards, dones, rtg, timesteps, mask, task_desc_emb, history_emb, strategy_emb = batch
        except Exception as e:
            logger.error(f"Data loading error: {e}")
            continue

        # Move tensors to device.
        states = states.to(device)
        actions = actions.to(device)
        rewards = rewards.to(device)
        dones = dones.to(device)
        rtg = rtg.to(device)
        timesteps = timesteps.to(device)
        mask = mask.to(device)
        task_desc_emb = task_desc_emb.to(device)
        history_emb = history_emb.to(device)
        strategy_emb = strategy_emb.to(device)

        # Forward pass and loss.
        loss = model.step(states, actions, rewards, dones, rtg, timesteps, mask,
                         language_task=task_desc_emb, language_history=history_emb, language_strategy=strategy_emb)

        # Logging.
        if step % 100 == 0:
            elapsed_time = time.time() - start_time
            logger.info(f"Step {step}/{args.num_steps} | Loss: {loss:.4f} | Time: {elapsed_time:.3f}s")

        # Save checkpoints.
        if (step + 1) % args.save_interval == 0:
            checkpoint_path = os.path.join(output_dir, f'checkpoint_{step+1}')
            os.makedirs(checkpoint_path, exist_ok=True)

            # Save model weights.
            torch.save(model.state_dict(), os.path.join(checkpoint_path, 'language_dt_2048.pt'))

            # Save training config.
            config = {
                'state_dim': 16,
                'act_dim': 1,
                'max_length': 20,
                'max_ep_len': 96,
                'use_language': args.use_language,
                'language_type': args.language_type,
                'language_emb_dim': args.language_emb_dim,
                'step': step + 1
            }

            import json
            with open(os.path.join(checkpoint_path, 'config.json'), 'w') as f:
                json.dump(config, f, indent=2)

            # Save normalization stats.
            if hasattr(dataset, 'state_mean') and hasattr(dataset, 'state_std'):
                normalize_dict = {
                    'state_mean': dataset.state_mean,
                    'state_std': dataset.state_std
                }
                save_normalize_dict(normalize_dict, os.path.join(checkpoint_path, 'normalize_dict.pkl'))

            logger.info(f"Checkpoint saved: {checkpoint_path}")

    logger.info("Training complete.")
    logger.info(f"Final model saved to: {output_dir}")

if __name__ == "__main__":
    main()
